
---

```markdown
# ðŸ§  Causal Narrative Physics Engine (CNPE)

**Track-B Submission | BDH-Driven Long-Horizon Narrative Reasoning**

CNPE is a causal reasoning system that determines whether a hypothetical character backstory is **globally consistent** with a full novel.  
Instead of relying on semantic similarity or retrieval, CNPE models the novel as a **persistent causal memory field** and evaluates backstories based on **energy stability** inside that field.

---

## ðŸ“Œ Problem Statement

Large Language Models often fail at **global narrative consistency**.  
A backstory can be locally fluent yet **globally impossible** due to:

- Timeline violations  
- Character contradictions  
- Geographic or causal impossibilities  

**Track-B** requires detecting such inconsistencies using **long-horizon reasoning**, not surface plausibility.

---

## ðŸ’¡ Core Idea

> **Narrative consistency is a causal physics problem.**

We treat the novel as a **causal field** learned by a persistent memory model.  
A backstory is consistent if it does **not inject high energy (friction)** into that memory.

---

## ðŸ§® Synaptic Friction Signal

We define a scalar reasoning signal:

\[
\Delta = \mathcal{L}_{fresh}(x) - \mathcal{L}_{memory}(x)
\]

Where:
- \( \mathcal{L}_{fresh} \): Loss from a BDH model with **no narrative memory**
- \( \mathcal{L}_{memory} \): Loss from a BDH model **primed on the novel**

### Interpretation
- **Low Î”** â†’ Stable under narrative constraints â†’ **Consistent**
- **High Î”** â†’ Violates narrative physics â†’ **Contradict**

---

## ðŸ—ï¸ Architecture

```

```
                  FULL NOVEL (100k+ tokens)
                           â”‚
                           â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚     BDH Memory Digestion    â”‚
             â”‚   (Hebbian Learning Mode)   â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Persistent Narrative Memory   â”‚
            â”‚  (Sparse Synaptic Causal Field) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                     â”‚
```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Fresh BDH        â”‚           â”‚       Primed BDH         â”‚
â”‚   (No narrative memory) â”‚           â”‚ (Narrative memory core)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â–¼
Î” = Loss_fresh âˆ’ Loss_primed
(Synaptic Friction Energy)
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Energy-Based Decision Layer   â”‚
â”‚  (Monotonic Physics Threshold) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
Final Consistency Label
Consistent (1) / Contradict (0)

```

---

## âš™ï¸ Implementation Workflow

| Step | Description |
|----|------------|
| Novel Digestion | Full novel processed once using BDH in learning mode |
| Memory Formation | Hebbian synapses encode long-range narrative constraints |
| Fresh Model | Measures baseline language surprise |
| Primed Model | Measures surprise under narrative memory |
| Friction Computation | Î” = Loss_fresh âˆ’ Loss_memory |
| Threshold Selection | Derived from training distribution |
| Classification | Low Î” â†’ Consistent, High Î” â†’ Contradict |
| Test Inference | Same physics rule applied to unseen data |

---

## âŒ Methods Tried & Rejected

| Method | Why It Failed |
|------|--------------|
| Embedding Similarity | Only captures surface semantics |
| RAG Pipelines | Retrieval noise destroyed causal signal |
| SVM / ML Classifiers | Overfit small, ambiguous dataset |
| Probabilistic Thresholds | Unstable under ambiguity |

These methods plateaued at **~50â€“55% accuracy** and lacked interpretability.

---

## ðŸ§  Why BDH Was Chosen

BDH (Baby Dragon Hatchling) supports:

- Persistent Hebbian memory  
- Long-context digestion  
- Incremental belief formation  

This makes it suitable for **causal memory modeling**, not just text generation.

---

## âš ï¸ Challenges Faced & Solutions

| Challenge | Solution |
|--------|---------|
| Dataset ambiguity | Treated as empirical performance ceiling |
| Retrieval noise | Removed RAG entirely |
| Torch runtime crashes | Clean PyTorch reinstall |
| Class imbalance | Physics-based monotonic classifier |
| Semantic similarity | Global causal reasoning |

---

## ðŸ“Š Results & Performance

| Metric | Value |
|------|------|
| Validation Accuracy | **~67%** |
| Stability | High |
| Interpretability | High |
| Overfitting | None |
| Model Type | Energy-based causal memory |

> The dataset is intentionally ambiguous; this accuracy represents an **empirical ceiling**, not underfitting.

---

## ðŸ“ˆ Key Visualizations

- Causal Energy Distribution (Î” vs labels)
- Learned Narrative Physics Field (test set)
- Easy vs Hard Reasoning Examples
- Precisionâ€“Recall vs Energy Threshold

These confirm the presence of a **monotonic causal physics signal**.

---

## ðŸ§¾ Final Claim

CNPE does **not** classify text using similarity.

It **simulates narrative physics** by measuring causal energy stability inside persistent BDH memory.

This makes it a true **Track-B compliant reasoning system**.

---

## ðŸ“‚ Repository Structure

```

â”œâ”€â”€ team_harry_puttar_jupyter.ipynb        # Full implementation
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ results.csv           # Final predictions
â””â”€â”€ docs/                 # Supporting documentation

```

---

## ðŸ”š Conclusion

CNPE reframes narrative consistency as a **physics-based causal reasoning problem**.  
By removing symbolic noise and focusing on memory stability, it achieves interpretable, stable, and empirically optimal performance.

---

**Author:** Shashi Bhushan Vijay  
**Track:** Track-B  
**Approach:** BDH-Driven Causal Memory
```

---

